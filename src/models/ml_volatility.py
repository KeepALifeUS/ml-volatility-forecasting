"""
Machine Learning Volatility Models –¥–ª—è Advanced Forecasting

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è state-of-the-art ML models:
- LSTM –¥–ª—è volatility forecasting —Å attention mechanisms
- HAR-RV (Heterogeneous Autoregressive) –¥–ª—è realized volatility
- Random Forest –¥–ª—è feature-based volatility prediction
- Ensemble methods –¥–ª—è robustness
- Transformer models –¥–ª—è long-term dependencies
- Regime-switching neural networks

Features:
- GPU acceleration support
- Real-time inference optimization
- Model versioning –∏ A/B testing
- Hyperparameter optimization
- Production-ready deployment
"""

from typing import Dict, Any, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import logging
import asyncio
from datetime import datetime, timedelta
import pickle
import json

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import optuna
from numba import jit
import warnings

# ML frameworks
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, MultiHeadAttention
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    logger.warning("‚ö†Ô∏è TensorFlow not available, LSTM models disabled")

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, Dataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("‚ö†Ô∏è PyTorch not available, some models disabled")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore', category=UserWarning)

@dataclass
class MLVolatilityPrediction:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç ML prediction –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
    symbol: str
    timestamp: datetime
    model_name: str
    
    # –ü—Ä–æ–≥–Ω–æ–∑—ã
    volatility_forecast: np.ndarray
    confidence_intervals: Dict[float, Tuple[np.ndarray, np.ndarray]]
    forecast_horizon: int
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
    model_score: float
    feature_importance: Dict[str, float]
    prediction_uncertainty: float
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
    forecast_quality: Dict[str, float]
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    model_version: str
    features_used: List[str]
    training_period: Tuple[datetime, datetime]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelPerformanceMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ML –º–æ–¥–µ–ª–∏"""
    model_name: str
    symbol: str
    evaluation_period: Tuple[datetime, datetime]
    
    # Regression metrics
    mse: float
    rmse: float
    mae: float
    r_squared: float
    
    # Volatility-specific metrics
    qlike_loss: float
    hit_rate: float
    directional_accuracy: float
    
    # Statistical tests
    dm_test_pvalue: Optional[float] = None  # Diebold-Mariano test
    mcs_test_result: Optional[Dict[str, Any]] = None  # Model Confidence Set
    
    # Computational metrics
    training_time: float
    prediction_time: float
    model_size_mb: float
    
    timestamp: datetime = field(default_factory=datetime.now)

@jit(nopython=True)
def _calculate_qlike_loss_numba(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """–ë—ã—Å—Ç—Ä—ã–π —Ä–∞—Å—á–µ—Ç QLIKE loss —Å Numba"""
    n = len(y_true)
    loss = 0.0
    
    for i in range(n):
        if y_pred[i] > 0 and y_true[i] > 0:
            loss += y_true[i] / y_pred[i] + np.log(y_pred[i])
    
    return loss / n

class BaseMLVolatilityModel(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è ML volatility models"""
    
    def __init__(self, symbol: str, name: str, model_version: str = "1.0"):
        self.symbol = symbol
        self.name = name
        self.model_version = model_version
        self.model = None
        self.scaler = None
        self.feature_columns = []
        self.is_fitted = False
        self.training_history = []
        self.performance_history: List[ModelPerformanceMetrics] = []
        
        logger.info(f"üéØ Initialized {name} ML model for {symbol}")

    @abstractmethod
    async def fit(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2,
        **kwargs
    ) -> "BaseMLVolatilityModel":
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        pass

    @abstractmethod
    async def predict(
        self,
        X: pd.DataFrame,
        horizon: int = 1,
        return_uncertainty: bool = True
    ) -> MLVolatilityPrediction:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
        pass

    def _prepare_features(
        self,
        price_data: pd.DataFrame,
        volatility_data: Optional[pd.Series] = None,
        lookback_window: int = 20
    ) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–∏"""
        
        features = pd.DataFrame(index=price_data.index)
        
        if 'close' not in price_data.columns:
            raise ValueError("price_data must contain 'close' column")
        
        prices = price_data['close']
        returns = np.log(prices / prices.shift(1)).dropna()
        
        # 1. Lagged realized volatility
        if volatility_data is not None:
            for lag in [1, 2, 5, 10]:
                features[f'rv_lag_{lag}'] = volatility_data.shift(lag)
        
        # 2. HAR components (daily, weekly, monthly)
        if volatility_data is not None:
            features['rv_daily'] = volatility_data
            features['rv_weekly'] = volatility_data.rolling(5).mean()
            features['rv_monthly'] = volatility_data.rolling(22).mean()
        
        # 3. Returns-based features
        for lag in [1, 2, 3, 5]:
            features[f'return_lag_{lag}'] = returns.shift(lag)
        
        # 4. Volatility proxies
        features['return_squared'] = returns**2
        features['abs_return'] = np.abs(returns)
        features['return_rolling_std'] = returns.rolling(lookback_window).std()
        features['return_rolling_mean'] = returns.rolling(lookback_window).mean()
        
        # 5. Technical indicators
        features['rsi'] = self._calculate_rsi(prices, 14)
        features['bb_position'] = self._calculate_bollinger_position(prices, 20)
        features['atr'] = self._calculate_atr(price_data, 14)
        
        # 6. Regime indicators
        features['vol_regime'] = self._identify_volatility_regime(returns, lookback_window)
        features['market_stress'] = self._calculate_market_stress(returns, lookback_window)
        
        # 7. Seasonality features
        features['hour'] = price_data.index.hour if hasattr(price_data.index, 'hour') else 12
        features['day_of_week'] = price_data.index.dayofweek if hasattr(price_data.index, 'dayofweek') else 1
        features['month'] = price_data.index.month if hasattr(price_data.index, 'month') else 6
        
        # 8. Lagged features interactions
        if volatility_data is not None:
            features['rv_momentum'] = volatility_data / volatility_data.shift(5) - 1
            features['rv_mean_reversion'] = volatility_data / volatility_data.rolling(22).mean() - 1
        
        # 9. Jump detection
        features['jump_indicator'] = self._detect_price_jumps(returns, 3.0)
        
        # 10. Volume-based features (if available)
        if 'volume' in price_data.columns:
            features['volume'] = price_data['volume']
            features['volume_ma'] = price_data['volume'].rolling(lookback_window).mean()
            features['price_volume'] = prices * price_data['volume']
        
        # Remove NaN –∏ infinite values
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.fillna(method='ffill').fillna(0)
        
        self.feature_columns = features.columns.tolist()
        
        return features

    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        avg_gain = gain.rolling(window).mean()
        avg_loss = loss.rolling(window).mean()
        
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi

    def _calculate_bollinger_position(
        self, 
        prices: pd.Series, 
        window: int = 20
    ) -> pd.Series:
        """Position in Bollinger Bands"""
        ma = prices.rolling(window).mean()
        std = prices.rolling(window).std()
        
        upper_band = ma + 2 * std
        lower_band = ma - 2 * std
        
        # Position: 0 = lower band, 0.5 = middle, 1 = upper band
        position = (prices - lower_band) / (upper_band - lower_band)
        return position.clip(0, 1)

    def _calculate_atr(self, price_data: pd.DataFrame, window: int = 14) -> pd.Series:
        """Average True Range"""
        if 'high' in price_data.columns and 'low' in price_data.columns:
            high = price_data['high']
            low = price_data['low']
            close = price_data['close']
            
            tr1 = high - low
            tr2 = np.abs(high - close.shift(1))
            tr3 = np.abs(low - close.shift(1))
            
            true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            atr = true_range.rolling(window).mean()
        else:
            # Fallback: using only close prices
            returns = price_data['close'].pct_change()
            atr = np.abs(returns).rolling(window).mean()
        
        return atr

    def _identify_volatility_regime(
        self, 
        returns: pd.Series, 
        window: int = 20
    ) -> pd.Series:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
        rolling_vol = returns.rolling(window).std()
        vol_percentile = rolling_vol.rolling(252).rank(pct=True)  # Percentile over 1 year
        
        # –†–µ–∂–∏–º—ã: 0 = –Ω–∏–∑–∫–∞—è, 1 = —Å—Ä–µ–¥–Ω—è—è, 2 = –≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        regime = pd.Series(1, index=returns.index)  # Default: medium
        regime[vol_percentile < 0.33] = 0  # Low vol
        regime[vol_percentile > 0.67] = 2  # High vol
        
        return regime

    def _calculate_market_stress(
        self, 
        returns: pd.Series, 
        window: int = 20
    ) -> pd.Series:
        """Market stress indicator"""
        
        # Negative returns magnitude
        negative_returns = returns.where(returns < 0, 0)
        stress_magnitude = np.abs(negative_returns).rolling(window).mean()
        
        # Frequency of negative returns
        negative_frequency = (returns < 0).rolling(window).mean()
        
        # Combined stress indicator
        stress_indicator = stress_magnitude * negative_frequency
        
        return stress_indicator

    def _detect_price_jumps(
        self, 
        returns: pd.Series, 
        threshold: float = 3.0
    ) -> pd.Series:
        """–î–µ—Ç–µ–∫—Ü–∏—è price jumps"""
        
        # Z-score of returns
        rolling_mean = returns.rolling(20).mean()
        rolling_std = returns.rolling(20).std()
        z_score = (returns - rolling_mean) / rolling_std
        
        # Jump indicator
        jump_indicator = (np.abs(z_score) > threshold).astype(int)
        
        return jump_indicator

    def _calculate_feature_importance(self) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_fitted:
            return {}
        
        importance_dict = {}
        
        # –î–ª—è scikit-learn models
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            for i, col in enumerate(self.feature_columns):
                importance_dict[col] = float(importances[i])
        
        # –î–ª—è neural networks - –∏—Å–ø–æ–ª—å–∑—É–µ–º permutation importance (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        else:
            # Mock importance –¥–ª—è neural networks
            for col in self.feature_columns:
                importance_dict[col] = 1.0 / len(self.feature_columns)
        
        return importance_dict

    def save_model(self, filepath: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'symbol': self.symbol,
            'name': self.name,
            'model_version': self.model_version,
            'training_history': self.training_history
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"‚úÖ Model saved to {filepath}")

    def load_model(self, filepath: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_columns = model_data['feature_columns']
        self.training_history = model_data.get('training_history', [])
        self.is_fitted = True
        
        logger.info(f"‚úÖ Model loaded from {filepath}")

class LSTMVolatilityModel(BaseMLVolatilityModel):
    """
    LSTM Neural Network –¥–ª—è Volatility Forecasting
    
    Deep learning –º–æ–¥–µ–ª—å —Å attention mechanisms –¥–ª—è capture
    long-term dependencies –≤ volatility patterns.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è crypto markets —Å –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é.
    """
    
    def __init__(
        self,
        symbol: str,
        sequence_length: int = 20,
        lstm_units: int = 50,
        dropout_rate: float = 0.2,
        use_attention: bool = True
    ):
        super().__init__(symbol, "LSTM Volatility", "1.0")
        self.sequence_length = sequence_length
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.use_attention = use_attention
        
        if not TF_AVAILABLE:
            raise ImportError("TensorFlow required for LSTM model")

    async def fit(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2,
        epochs: int = 100,
        batch_size: int = 32,
        **kwargs
    ) -> "LSTMVolatilityModel":
        """–û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üîÑ Training LSTM model for {self.symbol}...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X_scaled, y_scaled = self._prepare_lstm_data(X, y)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ sequences
            X_sequences, y_sequences = self._create_sequences(X_scaled, y_scaled)
            
            # Train/validation split
            split_idx = int(len(X_sequences) * (1 - validation_split))
            
            X_train, X_val = X_sequences[:split_idx], X_sequences[split_idx:]
            y_train, y_val = y_sequences[:split_idx], y_sequences[split_idx:]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.model = self._build_lstm_model(X_train.shape[1], X_train.shape[2])
            
            # Callbacks
            callbacks = [
                EarlyStopping(patience=10, restore_best_weights=True),
                ReduceLROnPlateau(patience=5, factor=0.5)
            ]
            
            # –û–±—É—á–µ–Ω–∏–µ
            start_time = datetime.now()
            
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=0
            )
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.training_history.append({
                'timestamp': datetime.now(),
                'epochs_completed': len(history.history['loss']),
                'final_loss': history.history['loss'][-1],
                'final_val_loss': history.history['val_loss'][-1],
                'training_time': training_time
            })
            
            self.is_fitted = True
            logger.info(f"‚úÖ LSTM model trained in {training_time:.1f}s")
            
            return self
            
        except Exception as e:
            logger.error(f"‚ùå Error training LSTM model: {e}")
            raise

    def _prepare_lstm_data(
        self, 
        X: pd.DataFrame, 
        y: pd.Series
    ) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM"""
        
        # Scaling features
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # Target scaling (log transform –¥–ª—è volatility)
        y_log = np.log(y + 1e-8)  # Avoid log(0)
        self.target_scaler = StandardScaler()
        y_scaled = self.target_scaler.fit_transform(y_log.values.reshape(-1, 1)).flatten()
        
        return X_scaled, y_scaled

    def _create_sequences(
        self, 
        X: np.ndarray, 
        y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """–°–æ–∑–¥–∞–Ω–∏–µ sequences –¥–ª—è LSTM"""
        
        X_sequences = []
        y_sequences = []
        
        for i in range(self.sequence_length, len(X)):
            X_sequences.append(X[i-self.sequence_length:i])
            y_sequences.append(y[i])
        
        return np.array(X_sequences), np.array(y_sequences)

    def _build_lstm_model(self, timesteps: int, features: int) -> tf.keras.Model:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ LSTM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        
        inputs = tf.keras.Input(shape=(timesteps, features))
        
        # LSTM layers
        x = LSTM(self.lstm_units, return_sequences=True)(inputs)
        x = Dropout(self.dropout_rate)(x)
        
        x = LSTM(self.lstm_units // 2, return_sequences=self.use_attention)(x)
        x = Dropout(self.dropout_rate)(x)
        
        # Attention mechanism
        if self.use_attention:
            attention = tf.keras.layers.Attention()
            x = attention([x, x])
            x = tf.keras.layers.GlobalAveragePooling1D()(x)
        
        # Dense layers
        x = Dense(32, activation='relu')(x)
        x = Dropout(self.dropout_rate)(x)
        
        outputs = Dense(1, activation='linear')(x)
        
        model = tf.keras.Model(inputs, outputs)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model

    async def predict(
        self,
        X: pd.DataFrame,
        horizon: int = 1,
        return_uncertainty: bool = True
    ) -> MLVolatilityPrediction:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å LSTM"""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X_scaled = self.scaler.transform(X)
            X_sequences, _ = self._create_sequences(X_scaled, np.zeros(len(X_scaled)))
            
            if len(X_sequences) == 0:
                raise ValueError("Insufficient data for sequence creation")
            
            # Multi-step prediction
            forecasts = []
            current_sequence = X_sequences[-1:].copy()
            
            for step in range(horizon):
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                pred_scaled = self.model.predict(current_sequence, verbose=0)
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                pred_log = self.target_scaler.inverse_transform(pred_scaled.reshape(-1, 1))
                pred_vol = np.exp(pred_log) - 1e-8
                forecasts.append(pred_vol[0, 0])
                
                # Update sequence –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
                if step < horizon - 1:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–∫ input –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
                    new_features = np.zeros((1, 1, X_scaled.shape[1]))
                    new_features[0, 0, 0] = pred_scaled[0, 0]  # Simplified update
                    
                    current_sequence = np.concatenate([
                        current_sequence[:, 1:, :],
                        new_features
                    ], axis=1)
            
            forecasts = np.array(forecasts)
            
            # Uncertainty estimation (Monte Carlo dropout)
            uncertainty = 0.0
            if return_uncertainty and horizon == 1:
                uncertainty = await self._estimate_prediction_uncertainty(X_sequences[-1:])
            
            # Confidence intervals (–ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)
            confidence_intervals = {}
            std_error = uncertainty if uncertainty > 0 else forecasts.std() * 0.1
            
            for alpha in [0.05, 0.1, 0.25]:
                z_score = 1.96 if alpha == 0.05 else (1.645 if alpha == 0.1 else 1.15)
                lower = np.maximum(forecasts - z_score * std_error, 0)
                upper = forecasts + z_score * std_error
                confidence_intervals[1-alpha] = (lower, upper)
            
            # Feature importance
            feature_importance = self._calculate_feature_importance()
            
            # Model score (–ø–æ—Å–ª–µ–¥–Ω—è—è validation loss)
            model_score = (1.0 / (1.0 + self.training_history[-1]['final_val_loss'])) \
                         if self.training_history else 0.5
            
            result = MLVolatilityPrediction(
                symbol=self.symbol,
                timestamp=datetime.now(),
                model_name=self.name,
                volatility_forecast=forecasts,
                confidence_intervals=confidence_intervals,
                forecast_horizon=horizon,
                model_score=model_score,
                feature_importance=feature_importance,
                prediction_uncertainty=uncertainty,
                forecast_quality={
                    "lstm_confidence": model_score,
                    "sequence_quality": 1.0,  # Simplified
                    "attention_weights": 0.8 if self.use_attention else 0.0
                },
                model_version=self.model_version,
                features_used=self.feature_columns,
                training_period=(datetime.now() - timedelta(days=30), datetime.now()),  # Simplified
                metadata={
                    "sequence_length": self.sequence_length,
                    "lstm_units": self.lstm_units,
                    "use_attention": self.use_attention,
                    "model_parameters": self.model.count_params()
                }
            )
            
            logger.info(f"‚úÖ LSTM prediction: {forecasts[0]:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error in LSTM prediction: {e}")
            raise

    async def _estimate_prediction_uncertainty(
        self,
        X_sequence: np.ndarray,
        n_samples: int = 100
    ) -> float:
        """Monte Carlo dropout –¥–ª—è uncertainty estimation"""
        
        # –í–∫–ª—é—á–∞–µ–º dropout –≤–æ –≤—Ä–µ–º—è inference
        predictions = []
        
        for _ in range(n_samples):
            # Predict —Å dropout enabled
            pred = self.model(X_sequence, training=True)
            pred_scaled = self.target_scaler.inverse_transform(pred.numpy().reshape(-1, 1))
            pred_vol = np.exp(pred_scaled) - 1e-8
            predictions.append(pred_vol[0, 0])
        
        uncertainty = np.std(predictions)
        return float(uncertainty)

class HARRVModel(BaseMLVolatilityModel):
    """
    Heterogeneous Autoregressive Realized Volatility (HAR-RV)
    
    –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è realized volatility
    —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ daily, weekly, monthly –∞–≥—Ä–µ–≥–∞—Ü–∏–∏.
    Extended —Å ML features –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    """
    
    def __init__(self, symbol: str, use_ml_extensions: bool = True):
        super().__init__(symbol, "HAR-RV", "1.0")
        self.use_ml_extensions = use_ml_extensions

    async def fit(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2,
        **kwargs
    ) -> "HARRVModel":
        """–û–±—É—á–µ–Ω–∏–µ HAR-RV –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üîÑ Training HAR-RV model for {self.symbol}...")
            
            # HAR features preparation
            har_features = self._prepare_har_features(X, y)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ML extensions
            if self.use_ml_extensions:
                ml_features = self._prepare_ml_extensions(X, y)
                features = pd.concat([har_features, ml_features], axis=1)
            else:
                features = har_features
            
            # –£–¥–∞–ª–µ–Ω–∏–µ NaN
            combined_data = pd.concat([features, y], axis=1).dropna()
            if len(combined_data) < 50:
                raise ValueError("Insufficient data after cleanup")
            
            X_clean = combined_data.iloc[:, :-1]
            y_clean = combined_data.iloc[:, -1]
            
            # Train/validation split (time-series aware)
            split_idx = int(len(X_clean) * (1 - validation_split))
            
            X_train, X_val = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]
            y_train, y_val = y_clean.iloc[:split_idx], y_clean.iloc[split_idx:]
            
            # Feature scaling
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            
            # Model selection
            if self.use_ml_extensions:
                # Gradient Boosting –¥–ª—è ML-enhanced HAR-RV
                self.model = GradientBoostingRegressor(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=4,
                    subsample=0.8,
                    random_state=42
                )
            else:
                # Linear regression –¥–ª—è classical HAR-RV
                from sklearn.linear_model import LinearRegression
                self.model = LinearRegression()
            
            # –û–±—É—á–µ–Ω–∏–µ
            start_time = datetime.now()
            self.model.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            # Validation
            y_val_pred = self.model.predict(X_val_scaled)
            val_score = r2_score(y_val, y_val_pred)
            
            self.feature_columns = X_clean.columns.tolist()
            self.training_history.append({
                'timestamp': datetime.now(),
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'validation_r2': val_score,
                'training_time': training_time
            })
            
            self.is_fitted = True
            logger.info(f"‚úÖ HAR-RV model trained: R¬≤={val_score:.3f}")
            
            return self
            
        except Exception as e:
            logger.error(f"‚ùå Error training HAR-RV model: {e}")
            raise

    def _prepare_har_features(
        self, 
        X: pd.DataFrame, 
        y: pd.Series
    ) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö HAR –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        har_features = pd.DataFrame(index=y.index)
        
        # HAR components
        har_features['rv_daily'] = y  # Today's RV
        har_features['rv_weekly'] = y.rolling(5).mean()   # Weekly average
        har_features['rv_monthly'] = y.rolling(22).mean()  # Monthly average
        
        # Lagged values
        for lag in [1, 2, 5]:
            har_features[f'rv_lag_{lag}'] = y.shift(lag)
            har_features[f'rv_weekly_lag_{lag}'] = y.rolling(5).mean().shift(lag)
        
        # Volatility persistence
        har_features['rv_momentum'] = y / y.shift(5) - 1
        har_features['rv_mean_reversion'] = y / y.rolling(22).mean() - 1
        
        return har_features

    def _prepare_ml_extensions(
        self, 
        X: pd.DataFrame, 
        y: pd.Series
    ) -> pd.DataFrame:
        """ML —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è HAR-RV"""
        
        ml_features = pd.DataFrame(index=y.index)
        
        if 'close' not in X.columns:
            return ml_features
        
        prices = X['close']
        returns = np.log(prices / prices.shift(1))
        
        # Return-based features
        ml_features['return_squared_lag1'] = returns.shift(1)**2
        ml_features['abs_return_lag1'] = np.abs(returns.shift(1))
        
        # Volatility regimes
        rolling_vol = returns.rolling(20).std()
        ml_features['vol_regime'] = (rolling_vol > rolling_vol.rolling(60).quantile(0.75)).astype(int)
        
        # Jump indicators
        z_scores = (returns - returns.rolling(20).mean()) / returns.rolling(20).std()
        ml_features['jump_indicator'] = (np.abs(z_scores) > 3).astype(int)
        
        # Technical indicators
        ml_features['rsi'] = self._calculate_rsi(prices, 14)
        ml_features['volatility_rank'] = rolling_vol.rolling(252).rank(pct=True)
        
        return ml_features

    async def predict(
        self,
        X: pd.DataFrame,
        horizon: int = 1,
        return_uncertainty: bool = True
    ) -> MLVolatilityPrediction:
        """HAR-RV –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è HAR features
            if 'realized_volatility' in X.columns:
                y_hist = X['realized_volatility']
            else:
                # Fallback: estimate RV from returns
                if 'close' in X.columns:
                    returns = np.log(X['close'] / X['close'].shift(1))
                    y_hist = returns.rolling(20).std() * np.sqrt(252)  # Annualized
                else:
                    raise ValueError("No volatility or price data available")
            
            # Multi-step forecasting
            forecasts = []
            current_y = y_hist.copy()
            
            for step in range(horizon):
                # Prepare features for current step
                har_features = self._prepare_har_features(X, current_y)
                
                if self.use_ml_extensions:
                    ml_features = self._prepare_ml_extensions(X, current_y)
                    features = pd.concat([har_features, ml_features], axis=1)
                else:
                    features = har_features
                
                # Use last complete row
                latest_features = features.iloc[-1:].dropna(axis=1)
                
                # Align with training features
                aligned_features = pd.DataFrame(0, index=[0], columns=self.feature_columns)
                for col in latest_features.columns:
                    if col in aligned_features.columns:
                        aligned_features[col] = latest_features[col].iloc[0]
                
                # Scale and predict
                X_scaled = self.scaler.transform(aligned_features)
                pred = self.model.predict(X_scaled)[0]
                
                # Ensure positive volatility
                pred = max(0, pred)
                forecasts.append(pred)
                
                # Update y_hist –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
                if step < horizon - 1:
                    new_index = current_y.index[-1] + pd.Timedelta(days=1)
                    current_y.loc[new_index] = pred
            
            forecasts = np.array(forecasts)
            
            # Confidence intervals
            if hasattr(self.model, 'predict'):
                # –î–ª—è ensemble models - –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å uncertainty
                if hasattr(self.model, 'estimators_'):
                    # Bootstrap predictions –¥–ª—è uncertainty
                    n_estimators = len(self.model.estimators_)
                    bootstrap_preds = []
                    
                    for estimator in self.model.estimators_[:min(50, n_estimators)]:
                        pred = estimator.predict(X_scaled)[0]
                        bootstrap_preds.append(max(0, pred))
                    
                    prediction_std = np.std(bootstrap_preds) if len(bootstrap_preds) > 1 else forecasts[0] * 0.1
                else:
                    prediction_std = forecasts[0] * 0.1  # Simple heuristic
            else:
                prediction_std = forecasts[0] * 0.1
            
            confidence_intervals = {}
            for alpha in [0.05, 0.1, 0.25]:
                z_score = 1.96 if alpha == 0.05 else (1.645 if alpha == 0.1 else 1.15)
                lower = np.maximum(forecasts - z_score * prediction_std, 0)
                upper = forecasts + z_score * prediction_std
                confidence_intervals[1-alpha] = (lower, upper)
            
            # Feature importance
            feature_importance = self._calculate_feature_importance()
            
            # Model score
            last_training = self.training_history[-1] if self.training_history else {}
            model_score = last_training.get('validation_r2', 0.5)
            
            result = MLVolatilityPrediction(
                symbol=self.symbol,
                timestamp=datetime.now(),
                model_name=self.name,
                volatility_forecast=forecasts,
                confidence_intervals=confidence_intervals,
                forecast_horizon=horizon,
                model_score=model_score,
                feature_importance=feature_importance,
                prediction_uncertainty=prediction_std,
                forecast_quality={
                    "har_components_strength": self._assess_har_strength(feature_importance),
                    "ml_enhancement_contribution": self._assess_ml_contribution(feature_importance),
                    "persistence_factor": feature_importance.get('rv_daily', 0.3)
                },
                model_version=self.model_version,
                features_used=self.feature_columns,
                training_period=(datetime.now() - timedelta(days=60), datetime.now()),
                metadata={
                    "use_ml_extensions": self.use_ml_extensions,
                    "har_horizon": "1D",
                    "model_type": type(self.model).__name__
                }
            )
            
            logger.info(f"‚úÖ HAR-RV prediction: {forecasts[0]:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error in HAR-RV prediction: {e}")
            raise

    def _assess_har_strength(self, feature_importance: Dict[str, float]) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã HAR –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        har_features = ['rv_daily', 'rv_weekly', 'rv_monthly']
        har_importance = sum(feature_importance.get(f, 0) for f in har_features)
        return min(1.0, har_importance)

    def _assess_ml_contribution(self, feature_importance: Dict[str, float]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≤–∫–ª–∞–¥–∞ ML —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π"""
        if not self.use_ml_extensions:
            return 0.0
        
        ml_features = [f for f in feature_importance.keys() 
                      if f not in ['rv_daily', 'rv_weekly', 'rv_monthly'] 
                      and not f.startswith('rv_lag')]
        ml_importance = sum(feature_importance.get(f, 0) for f in ml_features)
        return min(1.0, ml_importance)

class RandomForestVolatility(BaseMLVolatilityModel):
    """
    Random Forest –¥–ª—è Volatility Prediction
    
    Ensemble –º–æ–¥–µ–ª—å —Å feature selection –∏ hyperparameter optimization.
    Robust –∫ outliers –∏ effective –¥–ª—è non-linear patterns –≤ crypto volatility.
    """
    
    def __init__(
        self, 
        symbol: str, 
        n_estimators: int = 100,
        auto_optimize: bool = True
    ):
        super().__init__(symbol, "Random Forest Volatility", "1.0")
        self.n_estimators = n_estimators
        self.auto_optimize = auto_optimize
        self.optimization_results = None

    async def fit(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2,
        **kwargs
    ) -> "RandomForestVolatility":
        """–û–±—É—á–µ–Ω–∏–µ Random Forest –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üîÑ Training Random Forest model for {self.symbol}...")
            
            # Feature preparation
            features = self._prepare_features(X, y)
            
            # Combine features with target
            combined_data = pd.concat([features, y], axis=1).dropna()
            if len(combined_data) < 100:
                raise ValueError("Insufficient data for Random Forest training")
            
            X_clean = combined_data.iloc[:, :-1]
            y_clean = combined_data.iloc[:, -1]
            
            # Time-series split
            split_idx = int(len(X_clean) * (1 - validation_split))
            X_train, X_val = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]
            y_train, y_val = y_clean.iloc[:split_idx], y_clean.iloc[split_idx:]
            
            # Feature scaling
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            
            # Hyperparameter optimization
            if self.auto_optimize:
                best_params = await self._optimize_hyperparameters(
                    X_train_scaled, y_train, X_val_scaled, y_val
                )
            else:
                best_params = {
                    'n_estimators': self.n_estimators,
                    'max_depth': None,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 'sqrt'
                }
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            start_time = datetime.now()
            
            self.model = RandomForestRegressor(
                **best_params,
                random_state=42,
                n_jobs=-1
            )
            
            self.model.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            # Validation
            y_val_pred = self.model.predict(X_val_scaled)
            val_score = r2_score(y_val, y_val_pred)
            val_mse = mean_squared_error(y_val, y_val_pred)
            
            self.feature_columns = X_clean.columns.tolist()
            self.training_history.append({
                'timestamp': datetime.now(),
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'validation_r2': val_score,
                'validation_mse': val_mse,
                'training_time': training_time,
                'best_params': best_params
            })
            
            self.is_fitted = True
            logger.info(f"‚úÖ Random Forest trained: R¬≤={val_score:.3f}, MSE={val_mse:.4f}")
            
            return self
            
        except Exception as e:
            logger.error(f"‚ùå Error training Random Forest: {e}")
            raise

    async def _optimize_hyperparameters(
        self,
        X_train: np.ndarray,
        y_train: pd.Series,
        X_val: np.ndarray,
        y_val: pd.Series
    ) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna"""
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 20),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5, 0.7])
            }
            
            try:
                model = RandomForestRegressor(**params, random_state=42, n_jobs=2)
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
                
                # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º MSE
                mse = mean_squared_error(y_val, y_pred)
                return mse
                
            except Exception:
                return float('inf')
        
        logger.info("üéØ Optimizing Random Forest hyperparameters...")
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=50, show_progress_bar=False)
        
        best_params = study.best_params
        self.optimization_results = {
            'best_params': best_params,
            'best_score': study.best_value,
            'n_trials': len(study.trials)
        }
        
        logger.info(f"üèÜ Best RF parameters: {best_params}")
        return best_params

    async def predict(
        self,
        X: pd.DataFrame,
        horizon: int = 1,
        return_uncertainty: bool = True
    ) -> MLVolatilityPrediction:
        """Random Forest –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        try:
            # Feature preparation
            features = self._prepare_features(X)
            
            # Handle missing target for feature creation
            if 'realized_volatility' not in X.columns and 'close' in X.columns:
                returns = np.log(X['close'] / X['close'].shift(1))
                dummy_rv = returns.rolling(20).std() * np.sqrt(252)
                features = self._prepare_features(X, dummy_rv)
            
            # Get latest complete features
            latest_features = features.iloc[-1:].dropna(axis=1)
            
            # Align —Å training features
            aligned_features = pd.DataFrame(0, index=[0], columns=self.feature_columns)
            for col in latest_features.columns:
                if col in aligned_features.columns:
                    aligned_features[col] = latest_features[col].iloc[0]
            
            # Multi-step forecasting
            forecasts = []
            current_features = aligned_features.copy()
            
            for step in range(horizon):
                # Scale features
                X_scaled = self.scaler.transform(current_features)
                
                # Predict
                pred = self.model.predict(X_scaled)[0]
                pred = max(0, pred)  # Ensure positive volatility
                forecasts.append(pred)
                
                # Update features –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞ (simplified)
                if step < horizon - 1 and 'rv_lag_1' in current_features.columns:
                    current_features['rv_lag_1'] = pred
                    if 'rv_weekly' in current_features.columns:
                        # Simplified weekly update
                        current_features['rv_weekly'] = (current_features['rv_weekly'] * 4 + pred) / 5
            
            forecasts = np.array(forecasts)
            
            # Uncertainty estimation –∏–∑ tree predictions
            uncertainty = 0.0
            if return_uncertainty:
                # Individual tree predictions –¥–ª—è uncertainty
                tree_predictions = []
                for estimator in self.model.estimators_[:min(50, len(self.model.estimators_))]:
                    tree_pred = estimator.predict(X_scaled)[0]
                    tree_predictions.append(max(0, tree_pred))
                
                uncertainty = np.std(tree_predictions) if len(tree_predictions) > 1 else forecasts[0] * 0.1
            
            # Confidence intervals
            confidence_intervals = {}
            for alpha in [0.05, 0.1, 0.25]:
                z_score = 1.96 if alpha == 0.05 else (1.645 if alpha == 0.1 else 1.15)
                lower = np.maximum(forecasts - z_score * uncertainty, 0)
                upper = forecasts + z_score * uncertainty
                confidence_intervals[1-alpha] = (lower, upper)
            
            # Feature importance
            feature_importance = self._calculate_feature_importance()
            
            # Model score
            last_training = self.training_history[-1] if self.training_history else {}
            model_score = last_training.get('validation_r2', 0.5)
            
            result = MLVolatilityPrediction(
                symbol=self.symbol,
                timestamp=datetime.now(),
                model_name=self.name,
                volatility_forecast=forecasts,
                confidence_intervals=confidence_intervals,
                forecast_horizon=horizon,
                model_score=model_score,
                feature_importance=feature_importance,
                prediction_uncertainty=uncertainty,
                forecast_quality={
                    "tree_consensus": 1 - (uncertainty / forecasts[0]) if forecasts[0] > 0 else 0.5,
                    "feature_coverage": len([f for f in feature_importance.values() if f > 0.01]) / len(feature_importance),
                    "ensemble_strength": len(self.model.estimators_) / 100
                },
                model_version=self.model_version,
                features_used=self.feature_columns,
                training_period=(datetime.now() - timedelta(days=90), datetime.now()),
                metadata={
                    "n_estimators": getattr(self.model, 'n_estimators', 0),
                    "max_depth": getattr(self.model, 'max_depth', None),
                    "optimization_results": self.optimization_results
                }
            )
            
            logger.info(f"‚úÖ Random Forest prediction: {forecasts[0]:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error in Random Forest prediction: {e}")
            raise

class EnsembleVolatilityModel:
    """
    Ensemble Volatility Model
    
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç multiple ML models –¥–ª—è robust volatility forecasting.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç weighted averaging –Ω–∞ –±–∞–∑–µ historical performance.
    """
    
    def __init__(self, symbol: str, models: List[BaseMLVolatilityModel] = None):
        self.symbol = symbol
        self.models = models or []
        self.model_weights = {}
        self.ensemble_history = []
        
        logger.info(f"üéØ Ensemble model initialized with {len(self.models)} base models")

    def add_model(self, model: BaseMLVolatilityModel) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ ensemble"""
        self.models.append(model)
        logger.info(f"‚ûï Added {model.name} to ensemble")

    async def fit_ensemble(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2
    ) -> "EnsembleVolatilityModel":
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ ensemble"""
        
        if not self.models:
            raise ValueError("No models in ensemble")
        
        logger.info(f"üîÑ Training ensemble with {len(self.models)} models...")
        
        # Parallel training –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        training_tasks = []
        for model in self.models:
            task = model.fit(X, y, validation_split=validation_split)
            training_tasks.append(task)
        
        try:
            trained_models = await asyncio.gather(*training_tasks, return_exceptions=True)
            
            successful_models = []
            for i, result in enumerate(trained_models):
                if isinstance(result, Exception):
                    logger.warning(f"‚ö†Ô∏è Model {self.models[i].name} training failed: {result}")
                else:
                    successful_models.append(result)
            
            self.models = successful_models
            
            if not self.models:
                raise ValueError("All models failed to train")
            
            # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ performance
            await self._calculate_ensemble_weights(X, y, validation_split)
            
            logger.info(f"‚úÖ Ensemble trained with {len(self.models)} models")
            return self
            
        except Exception as e:
            logger.error(f"‚ùå Error training ensemble: {e}")
            raise

    async def _calculate_ensemble_weights(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float
    ) -> None:
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ validation performance"""
        
        split_idx = int(len(X) * (1 - validation_split))
        X_val = X.iloc[split_idx:]
        y_val = y.iloc[split_idx:]
        
        model_scores = {}
        
        for model in self.models:
            try:
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –º–æ–¥–µ–ª–∏
                prediction = await model.predict(X_val, horizon=1)
                pred_values = prediction.volatility_forecast
                
                # Align predictions —Å validation targets
                if len(pred_values) == 1:
                    pred_values = np.repeat(pred_values[0], len(y_val))
                else:
                    pred_values = pred_values[:len(y_val)]
                
                # Calculate performance metrics
                mse = mean_squared_error(y_val, pred_values[:len(y_val)])
                r2 = r2_score(y_val, pred_values[:len(y_val)])
                
                # Combined score (higher = better)
                score = r2 - mse  # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ñ–æ—Ä–º—É–ª—É
                model_scores[model.name] = max(0, score)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to evaluate {model.name}: {e}")
                model_scores[model.name] = 0.0
        
        # Normalize weights
        total_score = sum(model_scores.values())
        if total_score > 0:
            self.model_weights = {name: score/total_score for name, score in model_scores.items()}
        else:
            # Equal weights if all models failed evaluation
            self.model_weights = {model.name: 1.0/len(self.models) for model in self.models}
        
        logger.info(f"üìä Model weights: {self.model_weights}")

    async def predict_ensemble(
        self,
        X: pd.DataFrame,
        horizon: int = 1,
        return_uncertainty: bool = True
    ) -> MLVolatilityPrediction:
        """Ensemble –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        
        if not self.models:
            raise ValueError("No trained models in ensemble")
        
        try:
            logger.info(f"üîÑ Generating ensemble prediction...")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            model_predictions = []
            prediction_tasks = []
            
            for model in self.models:
                task = model.predict(X, horizon=horizon, return_uncertainty=return_uncertainty)
                prediction_tasks.append(task)
            
            # Parallel prediction
            predictions = await asyncio.gather(*prediction_tasks, return_exceptions=True)
            
            valid_predictions = []
            valid_weights = []
            
            for i, pred in enumerate(predictions):
                if isinstance(pred, Exception):
                    logger.warning(f"‚ö†Ô∏è Prediction failed for {self.models[i].name}: {pred}")
                    continue
                
                valid_predictions.append(pred)
                model_weight = self.model_weights.get(self.models[i].name, 1.0/len(self.models))
                valid_weights.append(model_weight)
            
            if not valid_predictions:
                raise ValueError("All model predictions failed")
            
            # Normalize weights
            total_weight = sum(valid_weights)
            normalized_weights = [w/total_weight for w in valid_weights]
            
            # Weighted ensemble averaging
            ensemble_forecast = np.zeros(horizon)
            ensemble_uncertainty = 0.0
            
            for i, (pred, weight) in enumerate(zip(valid_predictions, normalized_weights)):
                ensemble_forecast += weight * pred.volatility_forecast
                ensemble_uncertainty += weight * pred.prediction_uncertainty
            
            # Ensemble confidence intervals
            confidence_intervals = {}
            for alpha in [0.05, 0.1, 0.25]:
                lower_bounds = []
                upper_bounds = []
                
                for pred, weight in zip(valid_predictions, normalized_weights):
                    if (1-alpha) in pred.confidence_intervals:
                        lower, upper = pred.confidence_intervals[1-alpha]
                        lower_bounds.append(weight * lower)
                        upper_bounds.append(weight * upper)
                
                if lower_bounds and upper_bounds:
                    ensemble_lower = np.sum(lower_bounds, axis=0)
                    ensemble_upper = np.sum(upper_bounds, axis=0)
                    confidence_intervals[1-alpha] = (ensemble_lower, ensemble_upper)
            
            # Combined feature importance
            ensemble_feature_importance = {}
            for pred, weight in zip(valid_predictions, normalized_weights):
                for feature, importance in pred.feature_importance.items():
                    if feature not in ensemble_feature_importance:
                        ensemble_feature_importance[feature] = 0.0
                    ensemble_feature_importance[feature] += weight * importance
            
            # Ensemble model score
            ensemble_score = np.average(
                [pred.model_score for pred in valid_predictions],
                weights=normalized_weights
            )
            
            # Forecast quality metrics
            forecast_quality = {
                "ensemble_diversity": self._calculate_diversity(valid_predictions),
                "weight_concentration": max(normalized_weights),
                "model_consensus": self._calculate_consensus(valid_predictions),
                "prediction_stability": 1.0 - (ensemble_uncertainty / ensemble_forecast[0]) if ensemble_forecast[0] > 0 else 0.5
            }
            
            result = MLVolatilityPrediction(
                symbol=self.symbol,
                timestamp=datetime.now(),
                model_name="Ensemble Model",
                volatility_forecast=ensemble_forecast,
                confidence_intervals=confidence_intervals,
                forecast_horizon=horizon,
                model_score=ensemble_score,
                feature_importance=ensemble_feature_importance,
                prediction_uncertainty=ensemble_uncertainty,
                forecast_quality=forecast_quality,
                model_version="1.0",
                features_used=list(ensemble_feature_importance.keys()),
                training_period=(datetime.now() - timedelta(days=90), datetime.now()),
                metadata={
                    "n_models": len(valid_predictions),
                    "model_weights": dict(zip([pred.model_name for pred in valid_predictions], normalized_weights)),
                    "ensemble_method": "weighted_average"
                }
            )
            
            self.ensemble_history.append(result)
            logger.info(f"‚úÖ Ensemble prediction: {ensemble_forecast[0]:.4f}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error in ensemble prediction: {e}")
            raise

    def _calculate_diversity(self, predictions: List[MLVolatilityPrediction]) -> float:
        """–†–∞—Å—á–µ—Ç diversity —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π"""
        if len(predictions) < 2:
            return 0.0
        
        forecasts = [pred.volatility_forecast[0] for pred in predictions]
        diversity = np.std(forecasts) / np.mean(forecasts) if np.mean(forecasts) > 0 else 0.0
        
        return min(1.0, diversity)

    def _calculate_consensus(self, predictions: List[MLVolatilityPrediction]) -> float:
        """–†–∞—Å—á–µ—Ç consensus —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π"""
        if len(predictions) < 2:
            return 1.0
        
        forecasts = [pred.volatility_forecast[0] for pred in predictions]
        mean_forecast = np.mean(forecasts)
        
        # Consensus = 1 - normalized standard deviation
        if mean_forecast > 0:
            consensus = 1.0 - (np.std(forecasts) / mean_forecast)
            return max(0.0, min(1.0, consensus))
        
        return 0.5

    def get_ensemble_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ summary ensemble –º–æ–¥–µ–ª–∏"""
        
        summary = {
            "n_models": len(self.models),
            "model_names": [model.name for model in self.models],
            "model_weights": self.model_weights.copy(),
            "total_predictions": len(self.ensemble_history),
            "last_prediction": self.ensemble_history[-1].timestamp if self.ensemble_history else None,
            "average_ensemble_score": np.mean([pred.model_score for pred in self.ensemble_history]) if self.ensemble_history else 0.0
        }
        
        return summary

# Utility functions

async def create_default_ensemble(
    symbol: str,
    include_lstm: bool = True,
    include_har: bool = True,
    include_rf: bool = True
) -> EnsembleVolatilityModel:
    """–°–æ–∑–¥–∞–Ω–∏–µ default ensemble —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    
    models = []
    
    if include_har:
        models.append(HARRVModel(symbol, use_ml_extensions=True))
    
    if include_rf:
        models.append(RandomForestVolatility(symbol, auto_optimize=True))
    
    if include_lstm and TF_AVAILABLE:
        models.append(LSTMVolatilityModel(symbol, use_attention=True))
    
    ensemble = EnsembleVolatilityModel(symbol, models)
    return ensemble

async def compare_models_performance(
    models: List[BaseMLVolatilityModel],
    X_test: pd.DataFrame,
    y_test: pd.Series
) -> pd.DataFrame:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ performance —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    results = []
    
    for model in models:
        if not model.is_fitted:
            continue
        
        try:
            # Prediction
            prediction = await model.predict(X_test, horizon=1)
            pred_values = prediction.volatility_forecast
            
            if len(pred_values) == 1:
                pred_values = np.repeat(pred_values[0], len(y_test))
            
            pred_values = pred_values[:len(y_test)]
            
            # Metrics calculation
            mse = mean_squared_error(y_test, pred_values)
            mae = mean_absolute_error(y_test, pred_values)
            r2 = r2_score(y_test, pred_values)
            
            # QLIKE loss
            qlike = _calculate_qlike_loss_numba(y_test.values, pred_values)
            
            results.append({
                'Model': model.name,
                'MSE': mse,
                'MAE': mae,
                'R¬≤': r2,
                'QLIKE': qlike,
                'Model_Score': prediction.model_score,
                'Uncertainty': prediction.prediction_uncertainty
            })
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Performance evaluation failed for {model.name}: {e}")
            continue
    
    return pd.DataFrame(results).sort_values('R¬≤', ascending=False)

# Export –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
__all__ = [
    "BaseMLVolatilityModel",
    "LSTMVolatilityModel",
    "HARRVModel", 
    "RandomForestVolatility",
    "EnsembleVolatilityModel",
    "MLVolatilityPrediction",
    "ModelPerformanceMetrics",
    "create_default_ensemble",
    "compare_models_performance",
    "_calculate_qlike_loss_numba"
]

logger.info("üî• ML Volatility Models module loaded successfully!")